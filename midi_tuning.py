#!/usr/bin/env python3
"""
MIDIç²¾åº¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ & æœ€é©åŒ–ã•ã‚ŒãŸMIDIå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‚’å…ƒã«æ”¹è‰¯ç‰ˆã®MIDIå¤‰æ›ã‚’å®Ÿè¡Œ
"""

import os
import sys
import time
import json
import numpy as np
import librosa
import pretty_midi
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    from basic_pitch.inference import predict_and_save, predict
    from basic_pitch import ICASSP_2022_MODEL_PATH
    BASIC_PITCH_AVAILABLE = True
except ImportError:
    BASIC_PITCH_AVAILABLE = False


class OptimizedMIDIConverter:
    """æœ€é©åŒ–ã•ã‚ŒãŸMIDIå¤‰æ›å™¨"""
    
    def __init__(self, tuning_results: Dict = None):
        self.sample_rate = 22050
        self.tuning_results = tuning_results or {}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
        self.optimal_params = {
            "vocals": {
                "fmin": 80, "fmax": 1000, "threshold": 80, 
                "min_duration": 0.08, "note_tolerance": 1
            },
            "drums": {
                "delta": 0.15, "wait": 8, "pre_max": 2, "post_max": 2,
                "sensitivity": "high"
            },
            "bass": {
                "fmin": 40, "fmax": 300, "threshold": 50, 
                "min_duration": 0.2, "note_tolerance": 2
            },
            "other": {
                "fmin": 100, "fmax": 1200, "threshold": 100, 
                "min_duration": 0.05, "note_tolerance": 1
            }
        }
        
        print("ğŸ¯ OptimizedMIDIConverteråˆæœŸåŒ–å®Œäº†")
        if tuning_results:
            print("   ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœé©ç”¨æ¸ˆã¿")
    
    def load_tuning_results(self, results_file: str):
        """ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                self.tuning_results = json.load(f)
            
            # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
            for track_type in ["vocals", "drums", "bass", "other"]:
                if track_type in self.tuning_results:
                    best_config = self.tuning_results[track_type].get("best_config", {})
                    if best_config.get("name") != "none":
                        # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã‹ã‚‰æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                        all_results = self.tuning_results[track_type].get("all_results", {})
                        config_name = best_config["name"]
                        if config_name in all_results:
                            params = all_results[config_name].get("params", {})
                            if params:
                                self.optimal_params[track_type].update(params)
            
            print(f"âœ… ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿å®Œäº†: {results_file}")
            
        except Exception as e:
            print(f"âš ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    def convert_with_optimal_params(self, separated_files: Dict[str, str], output_dir: str = None) -> Dict[str, str]:
        """æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§MIDIå¤‰æ›"""
        if not separated_files:
            raise ValueError("åˆ†é›¢ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if output_dir is None:
            first_file = Path(next(iter(separated_files.values())))
            output_dir = first_file.parent.parent / "optimized_midi_tracks"
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ¼ æœ€é©åŒ–MIDIå¤‰æ›é–‹å§‹")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        
        midi_files = {}
        conversion_stats = {}
        
        for track_name, audio_file in separated_files.items():
            if not os.path.exists(audio_file):
                print(f"   âš ï¸ {track_name}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            print(f"\n   ğŸ”„ {track_name} æœ€é©åŒ–å¤‰æ›ä¸­...")
            
            try:
                start_time = time.time()
                
                if track_name in ["vocals", "other"]:
                    # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒˆãƒ©ãƒƒã‚¯: æœ€é©åŒ–+basic-pitchä½µç”¨
                    midi_file, stats = self._convert_melody_optimized(
                        audio_file, output_path, track_name
                    )
                elif track_name == "drums":
                    # ãƒ‰ãƒ©ãƒ ãƒˆãƒ©ãƒƒã‚¯: æœ€é©åŒ–
                    midi_file, stats = self._convert_drums_optimized(
                        audio_file, output_path, track_name
                    )
                elif track_name == "bass":
                    # ãƒ™ãƒ¼ã‚¹ãƒˆãƒ©ãƒƒã‚¯: æœ€é©åŒ–
                    midi_file, stats = self._convert_bass_optimized(
                        audio_file, output_path, track_name
                    )
                else:
                    # ãã®ä»–: æ¨™æº–æœ€é©åŒ–
                    midi_file, stats = self._convert_melody_optimized(
                        audio_file, output_path, track_name
                    )
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                if midi_file:
                    midi_files[track_name] = midi_file
                    conversion_stats[track_name] = {
                        **stats,
                        "processing_time": processing_time
                    }
                    print(f"      âœ… å¤‰æ›å®Œäº†: {Path(midi_file).name}")
                    print(f"         éŸ³ç¬¦æ•°: {stats.get('note_count', 0)}")
                    print(f"         å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                else:
                    print(f"      âŒ å¤‰æ›å¤±æ•—")
                    
            except Exception as e:
                print(f"      âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å¤‰æ›çµ±è¨ˆè¡¨ç¤º
        self._display_conversion_summary(conversion_stats)
        
        return midi_files
    
    def _convert_melody_optimized(self, audio_file: str, output_dir: Path, track_name: str) -> Tuple[Optional[str], Dict]:
        """æœ€é©åŒ–ãƒ¡ãƒ­ãƒ‡ã‚£å¤‰æ›ï¼ˆbasic-pitchå„ªå…ˆï¼‰"""
        stats = {"method": "unknown", "note_count": 0}
        
        # 1. basic-pitchè©¦è¡Œï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚‰ï¼‰
        if BASIC_PITCH_AVAILABLE:
            try:
                print(f"         basic-pitchè©¦è¡Œ...")
                midi_file, bp_stats = self._convert_with_basic_pitch(
                    audio_file, output_dir, track_name
                )
                if midi_file and bp_stats.get("note_count", 0) > 10:
                    stats = {**bp_stats, "method": "basic-pitch"}
                    return midi_file, stats
                else:
                    print(f"         basic-pitchçµæœä¸ååˆ†ã€librosaæœ€é©åŒ–ã«åˆ‡æ›¿")
            except Exception as e:
                print(f"         basic-pitchå¤±æ•—: {e}")
        
        # 2. librosaæœ€é©åŒ–
        try:
            print(f"         librosaæœ€é©åŒ–å®Ÿè¡Œ...")
            midi_file, lib_stats = self._convert_with_librosa_optimized(
                audio_file, output_dir, track_name
            )
            stats = {**lib_stats, "method": "librosa_optimized"}
            return midi_file, stats
            
        except Exception as e:
            print(f"         librosaæœ€é©åŒ–å¤±æ•—: {e}")
            return None, {"method": "failed", "error": str(e)}
    
    def _convert_with_basic_pitch(self, audio_file: str, output_dir: Path, track_name: str) -> Tuple[Optional[str], Dict]:
        """basic-pitchå¤‰æ›ï¼ˆä¿®æ­£æ¸ˆã¿APIï¼‰"""
        try:
            output_file = output_dir / f"{track_name}_bp.mid"
            
            # basic-pitchã§äºˆæ¸¬
            model_output, midi_data, note_events = predict(
                str(audio_file),
                ICASSP_2022_MODEL_PATH
            )
            
            # MIDIãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            pm_midi = pretty_midi.PrettyMIDI()
            instrument = pretty_midi.Instrument(program=0)  # Piano
            
            for note_event in note_events:
                pitch, start_time, end_time, confidence = note_event
                
                # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ0.5ä»¥ä¸Šï¼‰
                if confidence >= 0.5:
                    note = pretty_midi.Note(
                        velocity=int(80 + confidence * 40),  # ä¿¡é ¼åº¦ã«å¿œã˜ãŸãƒ™ãƒ­ã‚·ãƒ†ã‚£
                        pitch=int(pitch),
                        start=start_time,
                        end=end_time
                    )
                    instrument.notes.append(note)
            
            pm_midi.instruments.append(instrument)
            pm_midi.write(str(output_file))
            
            stats = {
                "note_count": len(instrument.notes),
                "confidence_filtered": sum(1 for ne in note_events if ne[3] >= 0.5),
                "total_detected": len(note_events),
                "avg_confidence": np.mean([ne[3] for ne in note_events]) if note_events else 0
            }
            
            return str(output_file), stats
            
        except Exception as e:
            raise Exception(f"basic-pitchå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _convert_with_librosa_optimized(self, audio_file: str, output_dir: Path, track_name: str) -> Tuple[Optional[str], Dict]:
        """librosaæœ€é©åŒ–å¤‰æ›"""
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        params = self.optimal_params.get(track_name, self.optimal_params["vocals"])
        
        # éŸ³å£°èª­ã¿è¾¼ã¿
        y, sr = librosa.load(audio_file, sr=self.sample_rate)
        
        # å‰å‡¦ç†ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
        y = librosa.effects.preemphasis(y, coef=0.97)
        
        # ãƒ”ãƒƒãƒæ¤œå‡ºï¼ˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨ï¼‰
        f0 = librosa.yin(y, 
                        fmin=params["fmin"], 
                        fmax=params["fmax"], 
                        sr=sr)
        
        # æ™‚é–“è»¸
        times = librosa.frames_to_time(np.arange(len(f0)), sr=sr)
        
        # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        f0_smooth = self._smooth_pitch(f0, window_size=5)
        
        # MIDIãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        midi_data = pretty_midi.PrettyMIDI()
        instrument = pretty_midi.Instrument(program=0)
        
        # éŸ³ç¬¦ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        notes = self._generate_notes_optimized(
            f0_smooth, times, params
        )
        
        for note_info in notes:
            note = pretty_midi.Note(
                velocity=note_info["velocity"],
                pitch=note_info["pitch"],
                start=note_info["start"],
                end=note_info["end"]
            )
            instrument.notes.append(note)
        
        midi_data.instruments.append(instrument)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_file = output_dir / f"{track_name}_opt.mid"
        midi_data.write(str(output_file))
        
        stats = {
            "note_count": len(notes),
            "avg_duration": np.mean([n["end"] - n["start"] for n in notes]) if notes else 0,
            "pitch_range": max([n["pitch"] for n in notes]) - min([n["pitch"] for n in notes]) if notes else 0
        }
        
        return str(output_file), stats
    
    def _smooth_pitch(self, f0: np.ndarray, window_size: int = 5) -> np.ndarray:
        """ãƒ”ãƒƒãƒã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        kernel = np.ones(window_size) / window_size
        f0_padded = np.pad(f0, (window_size//2, window_size//2), mode='edge')
        f0_smooth = np.convolve(f0_padded, kernel, mode='valid')
        
        # NaNå€¤ã¯å…ƒã®å€¤ã‚’ä¿æŒ
        nan_mask = np.isnan(f0)
        f0_smooth[nan_mask] = f0[nan_mask]
        
        return f0_smooth
    
    def _generate_notes_optimized(self, f0: np.ndarray, times: np.ndarray, params: Dict) -> List[Dict]:
        """æœ€é©åŒ–éŸ³ç¬¦ç”Ÿæˆ"""
        notes = []
        current_note = None
        note_start = None
        note_velocity = 80
        
        for i, (time, freq) in enumerate(zip(times, f0)):
            if freq > params["threshold"] and not np.isnan(freq):
                midi_note = int(librosa.hz_to_midi(freq))
                
                # éŸ³åŸŸåˆ¶é™
                if params.get("min_pitch"):
                    midi_note = max(params["min_pitch"], midi_note)
                if params.get("max_pitch"):
                    midi_note = min(params["max_pitch"], midi_note)
                
                tolerance = params.get("note_tolerance", 1)
                
                if current_note is None or abs(midi_note - current_note) > tolerance:
                    # å‰ã®éŸ³ç¬¦ã‚’ç¢ºå®š
                    if current_note is not None and note_start is not None:
                        duration = time - note_start
                        if duration >= params["min_duration"]:
                            notes.append({
                                "pitch": current_note,
                                "start": note_start,
                                "end": time,
                                "velocity": note_velocity
                            })
                    
                    # æ–°ã—ã„éŸ³ç¬¦é–‹å§‹
                    current_note = midi_note
                    note_start = time
                    # ãƒ™ãƒ­ã‚·ãƒ†ã‚£ã‚’å‘¨æ³¢æ•°ã«åŸºã¥ã„ã¦èª¿æ•´
                    note_velocity = min(127, max(40, int(80 + freq / 10)))
            else:
                # éŸ³ç¬¦çµ‚äº†
                if current_note is not None and note_start is not None:
                    duration = time - note_start
                    if duration >= params["min_duration"]:
                        notes.append({
                            "pitch": current_note,
                            "start": note_start,
                            "end": time,
                            "velocity": note_velocity
                        })
                    current_note = None
                    note_start = None
        
        # æœ€å¾Œã®éŸ³ç¬¦å‡¦ç†
        if current_note is not None and note_start is not None:
            notes.append({
                "pitch": current_note,
                "start": note_start,
                "end": times[-1],
                "velocity": note_velocity
            })
        
        return notes
    
    def _convert_drums_optimized(self, audio_file: str, output_dir: Path, track_name: str) -> Tuple[Optional[str], Dict]:
        """æœ€é©åŒ–ãƒ‰ãƒ©ãƒ å¤‰æ›"""
        params = self.optimal_params.get("drums", {})
        
        # éŸ³å£°èª­ã¿è¾¼ã¿
        y, sr = librosa.load(audio_file, sr=self.sample_rate)
        
        # ã‚ªãƒ³ã‚»ãƒƒãƒˆæ¤œå‡ºï¼ˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        onset_frames = librosa.onset.onset_detect(
            y=y, sr=sr, units='frames',
            hop_length=512,
            backtrack=True,
            pre_max=params.get("pre_max", 2),
            post_max=params.get("post_max", 2),
            pre_avg=3,
            post_avg=5,
            delta=params.get("delta", 0.15),
            wait=params.get("wait", 8)
        )
        
        onset_times = librosa.frames_to_time(onset_frames, sr=sr)
        
        # MIDIãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        midi_data = pretty_midi.PrettyMIDI()
        drum_instrument = pretty_midi.Instrument(program=0, is_drum=True)
        
        # æ”¹è‰¯ãƒ‰ãƒ©ãƒ åˆ†é¡
        hits = self._classify_drums_improved(y, sr, onset_times)
        
        for hit in hits:
            note = pretty_midi.Note(
                velocity=hit["velocity"],
                pitch=hit["midi_note"],
                start=hit["time"],
                end=hit["time"] + 0.1
            )
            drum_instrument.notes.append(note)
        
        midi_data.instruments.append(drum_instrument)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_file = output_dir / f"{track_name}_opt.mid"
        midi_data.write(str(output_file))
        
        stats = {
            "note_count": len(hits),
            "kick_count": sum(1 for h in hits if h["type"] == "kick"),
            "snare_count": sum(1 for h in hits if h["type"] == "snare"),
            "hihat_count": sum(1 for h in hits if h["type"] == "hihat")
        }
        
        return str(output_file), stats
    
    def _classify_drums_improved(self, y: np.ndarray, sr: int, onset_times: np.ndarray) -> List[Dict]:
        """æ”¹è‰¯ãƒ‰ãƒ©ãƒ åˆ†é¡"""
        hits = []
        
        for onset_time in onset_times:
            frame_idx = librosa.time_to_frames(onset_time, sr=sr, hop_length=512)
            
            # ã‚ˆã‚Šå¤§ããªåˆ†æçª“
            window_size = 4096
            start_sample = max(0, frame_idx * 512 - window_size // 2)
            end_sample = min(len(y), start_sample + window_size)
            window = y[start_sample:end_sample]
            
            if len(window) >= 1024:
                # ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†æ
                spectrum = np.abs(np.fft.fft(window, n=4096))
                freqs = np.fft.fftfreq(4096, 1/sr)
                
                # å‘¨æ³¢æ•°å¸¯åŸŸåˆ†å‰²ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                low_band = spectrum[0:80]       # 0-800Hz  (ã‚­ãƒƒã‚¯)
                mid_low = spectrum[80:200]      # 800-2000Hz (ã‚¹ãƒã‚¢ä½åŸŸ)
                mid_high = spectrum[200:400]    # 2000-4000Hz (ã‚¹ãƒã‚¢é«˜åŸŸ)
                high_band = spectrum[400:800]   # 4000-8000Hz (ãƒã‚¤ãƒãƒƒãƒˆ)
                
                # ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—
                low_energy = np.sum(low_band)
                mid_energy = np.sum(mid_low) + np.sum(mid_high)
                high_energy = np.sum(high_band)
                total_energy = low_energy + mid_energy + high_energy
                
                if total_energy > 0:
                    # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¯”ç‡
                    low_ratio = low_energy / total_energy
                    mid_ratio = mid_energy / total_energy
                    high_ratio = high_energy / total_energy
                    
                    # æ”¹è‰¯åˆ†é¡åŸºæº–
                    if low_ratio > 0.55 and low_energy > high_energy:
                        drum_type = "kick"
                        midi_note = 36
                        velocity = min(127, int(90 + low_ratio * 37))
                    elif high_ratio > 0.45 and high_energy > low_energy:
                        drum_type = "hihat"
                        midi_note = 42
                        velocity = min(127, int(70 + high_ratio * 57))
                    else:
                        drum_type = "snare"
                        midi_note = 38
                        velocity = min(127, int(80 + mid_ratio * 47))
                    
                    hits.append({
                        "time": onset_time,
                        "type": drum_type,
                        "midi_note": midi_note,
                        "velocity": velocity,
                        "energy_ratios": {
                            "low": low_ratio,
                            "mid": mid_ratio,
                            "high": high_ratio
                        }
                    })
        
        return hits
    
    def _convert_bass_optimized(self, audio_file: str, output_dir: Path, track_name: str) -> Tuple[Optional[str], Dict]:
        """æœ€é©åŒ–ãƒ™ãƒ¼ã‚¹å¤‰æ›"""
        params = self.optimal_params.get("bass", {})
        
        # éŸ³å£°èª­ã¿è¾¼ã¿
        y, sr = librosa.load(audio_file, sr=self.sample_rate)
        
        # ä½åŸŸãƒ•ã‚£ãƒ«ã‚¿å¼·åŒ–
        y_filtered = librosa.effects.preemphasis(y, coef=0.0)
        
        # ä½åŸŸå¼·èª¿ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ 
        from scipy import signal
        nyquist = sr / 2
        low_cutoff = 300 / nyquist  # 300Hzä»¥ä¸‹ã‚’å¼·èª¿
        b, a = signal.butter(4, low_cutoff, btype='low')
        y_bass_enhanced = signal.filtfilt(b, a, y_filtered)
        
        # ãƒ”ãƒƒãƒæ¤œå‡ºï¼ˆä½åŸŸç‰¹åŒ–ï¼‰
        f0 = librosa.yin(y_bass_enhanced, 
                        fmin=params.get("fmin", 40), 
                        fmax=params.get("fmax", 300), 
                        sr=sr)
        
        times = librosa.frames_to_time(np.arange(len(f0)), sr=sr)
        
        # MIDIãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        midi_data = pretty_midi.PrettyMIDI()
        bass_instrument = pretty_midi.Instrument(program=33)  # Electric Bass
        
        # ãƒ™ãƒ¼ã‚¹å°‚ç”¨éŸ³ç¬¦ç”Ÿæˆ
        notes = self._generate_bass_notes_optimized(f0, times, params)
        
        for note_info in notes:
            note = pretty_midi.Note(
                velocity=note_info["velocity"],
                pitch=note_info["pitch"],
                start=note_info["start"],
                end=note_info["end"]
            )
            bass_instrument.notes.append(note)
        
        midi_data.instruments.append(bass_instrument)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_file = output_dir / f"{track_name}_opt.mid"
        midi_data.write(str(output_file))
        
        stats = {
            "note_count": len(notes),
            "avg_pitch": np.mean([n["pitch"] for n in notes]) if notes else 0,
            "avg_duration": np.mean([n["end"] - n["start"] for n in notes]) if notes else 0
        }
        
        return str(output_file), stats
    
    def _generate_bass_notes_optimized(self, f0: np.ndarray, times: np.ndarray, params: Dict) -> List[Dict]:
        """ãƒ™ãƒ¼ã‚¹å°‚ç”¨æœ€é©åŒ–éŸ³ç¬¦ç”Ÿæˆ"""
        notes = []
        current_note = None
        note_start = None
        
        # ãƒ™ãƒ¼ã‚¹å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        threshold = params.get("threshold", 50)
        min_duration = params.get("min_duration", 0.2)
        note_tolerance = params.get("note_tolerance", 2)
        
        for i, (time, freq) in enumerate(zip(times, f0)):
            if freq > threshold and not np.isnan(freq):
                midi_note = int(librosa.hz_to_midi(freq))
                
                # ãƒ™ãƒ¼ã‚¹éŸ³åŸŸåˆ¶é™å¼·åŒ– (E1-E4: 28-64)
                midi_note = max(28, min(64, midi_note))
                
                if current_note is None or abs(midi_note - current_note) > note_tolerance:
                    # å‰ã®éŸ³ç¬¦ç¢ºå®š
                    if current_note is not None and note_start is not None:
                        duration = time - note_start
                        if duration >= min_duration:
                            # ãƒ™ãƒ­ã‚·ãƒ†ã‚£èª¿æ•´ï¼ˆä½éŸ³ã»ã©å¼·ãï¼‰
                            velocity = max(70, min(120, int(120 - (current_note - 28) * 1.5)))
                            notes.append({
                                "pitch": current_note,
                                "start": note_start,
                                "end": time,
                                "velocity": velocity
                            })
                    
                    current_note = midi_note
                    note_start = time
            else:
                # éŸ³ç¬¦çµ‚äº†
                if current_note is not None and note_start is not None:
                    duration = time - note_start
                    if duration >= min_duration:
                        velocity = max(70, min(120, int(120 - (current_note - 28) * 1.5)))
                        notes.append({
                            "pitch": current_note,
                            "start": note_start,
                            "end": time,
                            "velocity": velocity
                        })
                    current_note = None
                    note_start = None
        
        # æœ€å¾Œã®éŸ³ç¬¦å‡¦ç†
        if current_note is not None and note_start is not None:
            velocity = max(70, min(120, int(120 - (current_note - 28) * 1.5)))
            notes.append({
                "pitch": current_note,
                "start": note_start,
                "end": times[-1],
                "velocity": velocity
            })
        
        return notes
    
    def _display_conversion_summary(self, stats: Dict):
        """å¤‰æ›çµ±è¨ˆè¡¨ç¤º"""
        print(f"\nğŸ“Š æœ€é©åŒ–å¤‰æ›çµ±è¨ˆ")
        
        total_notes = 0
        total_time = 0
        
        for track, track_stats in stats.items():
            notes = track_stats.get("note_count", 0)
            method = track_stats.get("method", "unknown")
            proc_time = track_stats.get("processing_time", 0)
            
            total_notes += notes
            total_time += proc_time
            
            print(f"   {track}:")
            print(f"     éŸ³ç¬¦æ•°: {notes}")
            print(f"     æ‰‹æ³•: {method}")
            print(f"     å‡¦ç†æ™‚é–“: {proc_time:.2f}ç§’")
            
            # ãƒˆãƒ©ãƒƒã‚¯å›ºæœ‰çµ±è¨ˆ
            if "avg_confidence" in track_stats:
                print(f"     å¹³å‡ä¿¡é ¼åº¦: {track_stats['avg_confidence']:.3f}")
            if "kick_count" in track_stats:
                print(f"     ã‚­ãƒƒã‚¯: {track_stats['kick_count']}")
                print(f"     ã‚¹ãƒã‚¢: {track_stats['snare_count']}")
                print(f"     ãƒã‚¤ãƒãƒƒãƒˆ: {track_stats['hihat_count']}")
            if "avg_pitch" in track_stats:
                print(f"     å¹³å‡ãƒ”ãƒƒãƒ: {track_stats['avg_pitch']:.1f}")
        
        print(f"\n   ğŸ“ˆ ç·è¨ˆ:")
        print(f"     ç·éŸ³ç¬¦æ•°: {total_notes}")
        print(f"     ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        print(f"     å¹³å‡å‡¦ç†é€Ÿåº¦: {total_notes/max(total_time, 0.1):.1f}éŸ³ç¬¦/ç§’")
    
    def merge_optimized_tracks(self, midi_files: Dict[str, str], output_file: str = None) -> Optional[str]:
        """æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯çµ±åˆ"""
        if not midi_files:
            return None
        
        if output_file is None:
            first_file = Path(next(iter(midi_files.values())))
            output_file = first_file.parent / "optimized_merged_composition.mid"
        
        print(f"\nğŸµ æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯çµ±åˆ")
        print(f"   å‡ºåŠ›: {output_file}")
        
        try:
            merged_midi = pretty_midi.PrettyMIDI()
            
            # ãƒˆãƒ©ãƒƒã‚¯é †åºæœ€é©åŒ–
            track_order = ["bass", "drums", "other", "vocals"]
            
            for track_name in track_order:
                if track_name in midi_files:
                    midi_file = midi_files[track_name]
                    if os.path.exists(midi_file):
                        try:
                            track_midi = pretty_midi.PrettyMIDI(midi_file)
                            
                            for instrument in track_midi.instruments:
                                # ãƒˆãƒ©ãƒƒã‚¯åè¨­å®š
                                instrument.name = f"Optimized_{track_name}"
                                merged_midi.instruments.append(instrument)
                            
                            print(f"   âœ… {track_name}: çµ±åˆå®Œäº†")
                            
                        except Exception as e:
                            print(f"   âŒ {track_name}: çµ±åˆã‚¨ãƒ©ãƒ¼ - {e}")
            
            # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            merged_midi.write(str(output_file))
            
            # çµ±è¨ˆ
            total_instruments = len(merged_midi.instruments)
            total_notes = sum(len(inst.notes) for inst in merged_midi.instruments)
            duration = merged_midi.get_end_time()
            
            print(f"   ğŸ“Š çµ±åˆçµæœ:")
            print(f"     æ¥½å™¨æ•°: {total_instruments}")
            print(f"     ç·éŸ³ç¬¦æ•°: {total_notes}")
            print(f"     æ¼”å¥æ™‚é–“: {duration:.2f}ç§’")
            print(f"     éŸ³ç¬¦å¯†åº¦: {total_notes/max(duration, 1):.1f}éŸ³ç¬¦/ç§’")
            
            return str(output_file)
            
        except Exception as e:
            print(f"   âŒ çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            return None


def run_complete_optimization_pipeline():
    """å®Œå…¨æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ å®Œå…¨æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print("=" * 60)
    
    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
    work_dir = Path.home() / "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ" / "conversion_music"
    
    if not work_dir.exists():
        print(f"âŒ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {work_dir}")
        return
    
    # åˆ†é›¢æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    separated_dirs = list(work_dir.glob("*_separated"))
    
    if not separated_dirs:
        print("âŒ åˆ†é›¢æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    separated_dir = separated_dirs[0]
    track_dir = separated_dir / "htdemucs" / "input"
    
    if not track_dir.exists():
        print(f"âŒ ãƒˆãƒ©ãƒƒã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {track_dir}")
        return
    
    print(f"ğŸ“ å‡¦ç†å¯¾è±¡: {track_dir}")
    
    # åˆ†é›¢ãƒ•ã‚¡ã‚¤ãƒ«åé›†
    separated_files = {}
    for track_name in ["vocals", "drums", "bass", "other"]:
        track_file = track_dir / f"{track_name}.wav"
        if track_file.exists():
            separated_files[track_name] = str(track_file)
            print(f"   âœ… {track_name}: {track_file.name}")
        else:
            print(f"   âš ï¸ {track_name}: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
    
    if not separated_files:
        print("âŒ æœ‰åŠ¹ãªåˆ†é›¢ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # Step 1: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœç¢ºèª
    tuning_results_file = work_dir / "tuning_results" / "tuning_results.json"
    
    converter = OptimizedMIDIConverter()
    
    if tuning_results_file.exists():
        print(f"\nğŸ“Š ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿")
        converter.load_tuning_results(str(tuning_results_file))
    else:
        print(f"\nâš ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœãªã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨")
    
    # Step 2: æœ€é©åŒ–MIDIå¤‰æ›å®Ÿè¡Œ
    print(f"\nğŸ¼ æœ€é©åŒ–MIDIå¤‰æ›å®Ÿè¡Œ")
    start_time = time.time()
    
    try:
        midi_files = converter.convert_with_optimal_params(separated_files)
        
        if midi_files:
            # Step 3: æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯çµ±åˆ
            merged_file = converter.merge_optimized_tracks(midi_files)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"\nâœ… æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
            print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
            print(f"   ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(midi_files)}")
            
            if merged_file:
                print(f"   çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«: {Path(merged_file).name}")
                print(f"\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: éŸ³è‰²å¤‰æ› (Step 3)")
                print(f"   8bitå¤‰æ›ã¾ãŸã¯ã‚ªãƒ«ã‚´ãƒ¼ãƒ«å¤‰æ›ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            
            # å“è³ªè©•ä¾¡å®Ÿè¡Œ
            quality_report = evaluate_conversion_quality(midi_files, separated_files)
            print(f"\nğŸ“ˆ å“è³ªè©•ä¾¡å®Œäº†")
            
        else:
            print(f"âŒ MIDIãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


def evaluate_conversion_quality(midi_files: Dict[str, str], original_files: Dict[str, str]) -> Dict:
    """å¤‰æ›å“è³ªè©•ä¾¡"""
    print(f"\nğŸ“Š å¤‰æ›å“è³ªè©•ä¾¡")
    
    quality_metrics = {}
    
    for track_name, midi_file in midi_files.items():
        if not os.path.exists(midi_file):
            continue
        
        original_file = original_files.get(track_name)
        if not original_file or not os.path.exists(original_file):
            continue
        
        print(f"   {track_name} è©•ä¾¡ä¸­...")
        
        try:
            # MIDIåˆ†æ
            midi_data = pretty_midi.PrettyMIDI(midi_file)
            total_notes = sum(len(inst.notes) for inst in midi_data.instruments)
            midi_duration = midi_data.get_end_time()
            
            # åŸéŸ³å£°åˆ†æ
            y, sr = librosa.load(original_file, sr=22050)
            audio_duration = len(y) / sr
            
            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            metrics = {
                "midi_notes": total_notes,
                "midi_duration": midi_duration,
                "audio_duration": audio_duration,
                "duration_match": abs(midi_duration - audio_duration) / audio_duration,
                "note_density": total_notes / max(midi_duration, 1),
                "file_size_kb": os.path.getsize(midi_file) / 1024
            }
            
            # ãƒˆãƒ©ãƒƒã‚¯å›ºæœ‰è©•ä¾¡
            if track_name in ["vocals", "other"]:
                # ãƒ¡ãƒ­ãƒ‡ã‚£è©•ä¾¡
                if midi_data.instruments:
                    pitches = [note.pitch for inst in midi_data.instruments for note in inst.notes]
                    if pitches:
                        metrics["pitch_range"] = max(pitches) - min(pitches)
                        metrics["avg_pitch"] = np.mean(pitches)
                        
                        # éŸ³ç¨‹å¤‰åŒ–åˆ†æ
                        pitch_changes = [abs(pitches[i] - pitches[i-1]) for i in range(1, len(pitches))]
                        metrics["avg_pitch_change"] = np.mean(pitch_changes) if pitch_changes else 0
                        
            elif track_name == "drums":
                # ãƒ‰ãƒ©ãƒ è©•ä¾¡
                if midi_data.instruments and midi_data.instruments[0].is_drum:
                    drum_notes = midi_data.instruments[0].notes
                    kick_notes = [n for n in drum_notes if n.pitch == 36]
                    snare_notes = [n for n in drum_notes if n.pitch == 38]
                    hihat_notes = [n for n in drum_notes if n.pitch == 42]
                    
                    metrics["kick_ratio"] = len(kick_notes) / max(len(drum_notes), 1)
                    metrics["snare_ratio"] = len(snare_notes) / max(len(drum_notes), 1)
                    metrics["hihat_ratio"] = len(hihat_notes) / max(len(drum_notes), 1)
                    metrics["rhythm_regularity"] = calculate_rhythm_regularity(drum_notes)
                    
            elif track_name == "bass":
                # ãƒ™ãƒ¼ã‚¹è©•ä¾¡
                if midi_data.instruments:
                    bass_notes = [note for inst in midi_data.instruments for note in inst.notes]
                    if bass_notes:
                        pitches = [note.pitch for note in bass_notes]
                        durations = [note.end - note.start for note in bass_notes]
                        
                        metrics["avg_bass_pitch"] = np.mean(pitches)
                        metrics["bass_range"] = max(pitches) - min(pitches)
                        metrics["avg_note_duration"] = np.mean(durations)
                        metrics["bass_consistency"] = 1 - (np.std(pitches) / max(np.mean(pitches), 1))
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = calculate_quality_score(metrics, track_name)
            metrics["quality_score"] = quality_score
            
            quality_metrics[track_name] = metrics
            
            print(f"     éŸ³ç¬¦æ•°: {total_notes}")
            print(f"     æ™‚é–“ç²¾åº¦: {(1-metrics['duration_match'])*100:.1f}%")
            print(f"     å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.1f}/100")
            
        except Exception as e:
            print(f"     âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            quality_metrics[track_name] = {"error": str(e)}
    
    # ç·åˆè©•ä¾¡
    valid_scores = [m["quality_score"] for m in quality_metrics.values() 
                   if "quality_score" in m]
    
    if valid_scores:
        overall_score = np.mean(valid_scores)
        print(f"\nğŸ¯ ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {overall_score:.1f}/100")
        
        if overall_score >= 80:
            print("   ğŸŒŸ å„ªç§€ãªå¤‰æ›å“è³ª!")
        elif overall_score >= 60:
            print("   âœ… è‰¯å¥½ãªå¤‰æ›å“è³ª")
        elif overall_score >= 40:
            print("   âš ï¸ æ”¹å–„ã®ä½™åœ°ã‚ã‚Š")
        else:
            print("   ğŸ”§ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†èª¿æ•´ã‚’æ¨å¥¨")
    
    return quality_metrics


def calculate_rhythm_regularity(drum_notes: List) -> float:
    """ãƒªã‚ºãƒ è¦å‰‡æ€§è¨ˆç®—"""
    if len(drum_notes) < 3:
        return 0.0
    
    # éŸ³ç¬¦é–“éš”è¨ˆç®—
    intervals = []
    sorted_notes = sorted(drum_notes, key=lambda n: n.start)
    
    for i in range(1, len(sorted_notes)):
        interval = sorted_notes[i].start - sorted_notes[i-1].start
        intervals.append(interval)
    
    if not intervals:
        return 0.0
    
    # é–“éš”ã®ä¸€è²«æ€§ï¼ˆæ¨™æº–åå·®ã®é€†æ•°ï¼‰
    std_interval = np.std(intervals)
    mean_interval = np.mean(intervals)
    
    if mean_interval == 0:
        return 0.0
    
    # è¦å‰‡æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
    regularity = max(0, 1 - (std_interval / mean_interval))
    return regularity


def calculate_quality_score(metrics: Dict, track_type: str) -> float:
    """å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = 0.0
    
    # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæ™‚é–“ç²¾åº¦ï¼‰
    duration_match = metrics.get("duration_match", 1.0)
    time_score = max(0, (1 - duration_match) * 30)
    score += time_score
    
    # éŸ³ç¬¦å¯†åº¦ã‚¹ã‚³ã‚¢
    note_density = metrics.get("note_density", 0)
    if track_type in ["vocals", "other"]:
        # ãƒ¡ãƒ­ãƒ‡ã‚£: 1-8éŸ³ç¬¦/ç§’ãŒç†æƒ³
        if 1 <= note_density <= 8:
            density_score = 25
        else:
            density_score = max(0, 25 - abs(note_density - 4) * 3)
    elif track_type == "drums":
        # ãƒ‰ãƒ©ãƒ : 0.5-5ãƒ’ãƒƒãƒˆ/ç§’ãŒç†æƒ³
        if 0.5 <= note_density <= 5:
            density_score = 25
        else:
            density_score = max(0, 25 - abs(note_density - 2.5) * 5)
    elif track_type == "bass":
        # ãƒ™ãƒ¼ã‚¹: 0.3-3éŸ³ç¬¦/ç§’ãŒç†æƒ³
        if 0.3 <= note_density <= 3:
            density_score = 25
        else:
            density_score = max(0, 25 - abs(note_density - 1.5) * 8)
    else:
        density_score = 15
    
    score += density_score
    
    # ãƒˆãƒ©ãƒƒã‚¯å›ºæœ‰ã‚¹ã‚³ã‚¢
    if track_type in ["vocals", "other"]:
        # éŸ³åŸŸã‚¹ã‚³ã‚¢
        pitch_range = metrics.get("pitch_range", 0)
        if pitch_range >= 24:  # 2ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ä»¥ä¸Š
            range_score = 20
        elif pitch_range >= 12:  # 1ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ä»¥ä¸Š
            range_score = 15
        else:
            range_score = pitch_range * 15 / 12
        score += range_score
        
        # éŸ³ç¨‹å¤‰åŒ–ã‚¹ã‚³ã‚¢
        avg_change = metrics.get("avg_pitch_change", 0)
        if 1 <= avg_change <= 5:  # é©åº¦ãªéŸ³ç¨‹å¤‰åŒ–
            change_score = 15
        else:
            change_score = max(0, 15 - abs(avg_change - 3) * 3)
        score += change_score
        
    elif track_type == "drums":
        # ãƒ‰ãƒ©ãƒ åˆ†é¡ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        kick_ratio = metrics.get("kick_ratio", 0)
        snare_ratio = metrics.get("snare_ratio", 0)
        hihat_ratio = metrics.get("hihat_ratio", 0)
        
        balance_score = 0
        if 0.15 <= kick_ratio <= 0.45:
            balance_score += 8
        if 0.10 <= snare_ratio <= 0.40:
            balance_score += 8
        if 0.25 <= hihat_ratio <= 0.70:
            balance_score += 9
        
        score += balance_score
        
        # ãƒªã‚ºãƒ è¦å‰‡æ€§ã‚¹ã‚³ã‚¢
        regularity = metrics.get("rhythm_regularity", 0)
        score += regularity * 10
        
    elif track_type == "bass":
        # ãƒ™ãƒ¼ã‚¹ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        consistency = metrics.get("bass_consistency", 0)
        score += consistency * 20
        
        # éŸ³ç¬¦é•·ã‚¹ã‚³ã‚¢
        avg_duration = metrics.get("avg_note_duration", 0)
        if 0.2 <= avg_duration <= 1.0:
            duration_score = 15
        else:
            duration_score = max(0, 15 - abs(avg_duration - 0.6) * 10)
        score += duration_score
    
    return min(100, max(0, score))


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ MIDIå¤‰æ›æœ€é©åŒ– & å®Ÿè¡Œ")
    print("=" * 50)
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³é¸æŠ
    print("\nå®Ÿè¡Œã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
    print("1. ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–)")
    print("2. æœ€é©åŒ–å¤‰æ›å®Ÿè¡Œ (ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœé©ç”¨)")
    print("3. å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + å¤‰æ›)")
    
    try:
        choice = input("\né¸æŠã—ã¦ãã ã•ã„ (1-3): ").strip()
        
        if choice == "1":
            # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            print("\nğŸ”¬ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ...")
            os.system("python midi_tuning.py")
            
        elif choice == "2":
            # æœ€é©åŒ–å¤‰æ›ã®ã¿å®Ÿè¡Œ
            run_complete_optimization_pipeline()
            
        elif choice == "3":
            # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            print("\nğŸ”¬ Step 1: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ...")
            os.system("python midi_tuning.py")
            
            print("\nğŸ¼ Step 2: æœ€é©åŒ–å¤‰æ›å®Ÿè¡Œ...")
            run_complete_optimization_pipeline()
            
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()